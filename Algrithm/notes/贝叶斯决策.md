# 后验概率

​		在贝叶斯决策论中，后验概率（posterior probability）指在观测到一些**证据（evidence）**后，对于一个<u>假设或者事件发生的概率</u>进行修正后的概率。

​		具体来说，设 $H$ 是一个事件或假设，$E$ 是一些观测到的证据，那么根据贝叶斯定理，后验概率 $P(H|E)$ 可以通过先验概率 $P(H)$ 和似然函数 $P(E|H)$ 来计算：



其中，分母 $P(E)$ 是一个**归一化常量**，确保后验概率的总和为1，它可以通过全概率公式计算：



​		后验概率的意义是，在我们已经观测到一些证据 $E$ 之后，事件或假设 $H$ 发生的概率是多少。与先验概率相比，后验概率融合了先验知识和新的证据信息，因此可以更准确地反映实际情况。

​		后验概率在贝叶斯决策论中非常重要，因为它可以帮助我们确定最优的决策。在做出决策之前，我们可以计算每个假设的后验概率，然后选择具有最大后验概率的假设作为最终决策。

# 证据

​		在贝叶斯决策论中，证据（evidence）是指我们所观测到或收集到的<u>数据、事实、信息</u>等，用来对假设或事件的概率进行修正。可以理解为是<u>对观测到的现象的描述和测量</u>。

​		证据可以是<u>直接的观测数据</u>，也可以是通过数据处理和分析得到的<u>特征、统计量</u>等。在贝叶斯决策论中，我们使用证据来更新先验概率，得到后验概率，从而进行最优决策。

​		举个例子，假设我们要判断一枚硬币正反面的概率，先验概率是0.5，即认为硬币正反面概率相等。我们抛了10次硬币，观测到有7次正面朝上。这些观测数据就是证据，我们可以用它来更新先验概率，得到后验概率。具体来说，我们可以使用贝叶斯定理来计算：



​		其中，$P(\text{7次正面}|\text{正面})$ 是似然函数，$P(\text{正面})$ 是先验概率，$P(\text{7次正面})$ 是归一化常量。通过计算可以得到后验概率 $P(\text{正面}| \text{7次正面})$，它就是我们对硬币正反面概率的新的估计。

# 条件风险

​		在贝叶斯决策论中，条件风险（conditional risk）是指在<u>已知某个样本</u>的情况下，采取不同**决策**所产生的风险。它通常被定义为在给定样本 $x$ 的情况下，决策 $d_i$ 所产生的风险 $R(d_i|x)$。

​		具体来说，假设我们有一个分类问题，样本的特征表示为 $x$，有 $K$ 种可能的类别，决策空间为 $D={d_1,d_2,\cdots,d_K}$。对于任意样本 $x$ 和决策 $d_i$，我们可以定义一个损失函数 $L(d_i|x)$，用来表示在将样本 $x$ 分类为 $d_i$ 时所产生的损失。条件风险 $R(d_i|x)$ 通常被定义为对于给定样本 $x$，在采取决策 $d_i$ 时所期望的损失：



​		其中，期望值 $\mathbb{E}$ 是针对样本 $x$ 的分布进行的。因此，条件风险是一个对于每个样本和每个决策都有一个确定的值的函数。

​		在贝叶斯决策论中，我们通常希望选择能够最小化条件风险的决策，即：



​		其中 $d^*$ 表示最优决策。在进行决策时，我们会将已知样本的特征 $x$ 输入模型，计算每个决策 $d_i$ 的条件风险 $R(d_i|x)$，并选择最小化条件风险的决策 $d^*$。

​		需要注意的是，条件风险是对于每个样本和每个决策都有一个确定的值的函数，因此对于不同的样本，最优决策可能是不同的。因此，在实际应用中，我们需要对多个样本进行决策，并考虑整体风险，以达到最优的决策效果。

# 条件错误率

​		条件错误率（conditional error rate）是指在贝叶斯分类器中，对于某个类别 $C_i$，将它误判为其他类别的概率。它是衡量分类器性能的一个重要指标之一。

​		举个例子，假设我们有一个二分类问题，其中 $C_1$ 表示正例，$C_2$ 表示反例。如果在所有真实为 $C_1$ 的样本中，有 $10%$ 的样本被错误地分类为 $C_2$，那么 $C_1$ 的条件错误率为 $10%$。

​		可以这样理解条件错误率：对于某个类别 $C_i$，分类器在判断一个真实属于 $C_i$ 的样本时，错误地将其分类为其他类别的概率。

​		需要注意的是，条件错误率只考虑了将某个类别误判为其他类别的情况，它并没有考虑将其他类别误判为该类别的情况。因此，在评估分类器性能时，我们需要综合考虑各个类别的条件错误率，以及它们在整个数据集中的比例等因素。

# 最小距离分类器

​		最小距离分类器（Minimum Distance Classifier）是一种基于样本之间距离的分类方法。它根据每个样本点与各类别的距离来判断样本所属的类别。

​		具体地，对于一个待分类样本 $x$，我们需要计算它与各个类别的样本的距离，然后将它归为距离最近的类别中。这里的距离可以使用欧几里得距离、曼哈顿距离等常见的距离度量方式。

​		最小距离分类器简单直观，但在实际应用中，由于它只考虑了距离，对于复杂的分类问题效果有限。

# 线性分类器

​		线性分类器（Linear Classifier）是一种基于线性分割的分类方法。它将特征空间中的样本用一个超平面分成不同的类别。

​		具体地，对于二分类问题，线性分类器试图找到一个超平面 $w^Tx+b=0$，其中 $w$ 是法向量，$b$ 是截距，将特征空间中的样本点分成两类，即超平面两侧的点分别归为不同的类别。

​		在多分类问题中，可以使用多个超平面进行分类。线性分类器具有较强的可解释性和适用性，在处理高维数据时具有优势，但对于非线性分类问题，其表现可能会受到限制。



在基于正态分布的Bayes决策中，判别函数可以表示为样本 $x$ 属于类别 $i$ 的概率密度函数，即：

$$P(C_i|x) = \frac{p(x|C_i)P(C_i)}{p(x)}$$

其中 $p(x|C_i)$ 表示样本 $x$ 在类别 $C_i$ 中的条件概率密度函数，$P(C_i)$ 表示类别 $C_i$ 的先验概率，$p(x)$ 表示样本 $x$ 的概率密度函数。当 $p(x|C_i)$ 服从正态分布时，我们可以使用正态分布的概率密度函数公式进行计算。

在特例情况下，我们假设所有类别的先验概率相等，即 $P(C_1) = P(C_2) = \cdots = P(C_K)$。此时，我们可以将判别函数简化为：

$$f_i(x) = -\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i) - \frac{1}{2}\ln|\Sigma_i| + \ln P(C_i)$$

其中 $\mu_i$ 和 $\Sigma_i$ 分别是类别 $C_i$ 的均值向量和协方差矩阵，$|\Sigma_i|$ 表示协方差矩阵的行列式。这个简化形式也被称为线性判别函数。

在一般情况下，我们需要对样本的先验概率和各个类别的均值和协方差矩阵进行估计，然后使用判别函数计算样本属于各个类别的后验概率，并将样本归为后验概率最大的那个类别。

需要注意的是，正态分布假设只是Bayes决策中的一种特例，实际应用中，我们需要根据具体问题和数据集的情况选择适合的概率分布模型进行建模和计算。





在基于正态分布的贝叶斯决策中，判别函数是用来计算样本属于某个类别的后验概率的函数。当样本特征服从正态分布时，判别函数可以通过直接计算概率密度函数得到。

对于二分类问题，假设有两个类别 $C_1$ 和 $C_2$，它们的特征分布分别为 $N(\mu_1, \Sigma_1)$ 和 $N(\mu_2, \Sigma_2)$。根据贝叶斯公式，可以得到样本属于 $C_1$ 类的后验概率为：

$$p(C_1|\boldsymbol{x}) = \frac{p(\boldsymbol{x}|C_1)p(C_1)}{p(\boldsymbol{x})}$$

其中，$p(\boldsymbol{x}|C_1)$ 表示样本在给定类别 $C_1$ 的条件下的概率密度函数，也就是：

$$p(\boldsymbol{x}|C_1) = \frac{1}{(2\pi)^{d/2}|\Sigma_1|^{1/2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_1)^T\Sigma_1^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_1)\right)$$

其中，$d$ 表示样本的特征维度，$\boldsymbol{\mu}_1$ 和 $\Sigma_1$ 分别是 $C_1$ 类别的均值向量和协方差矩阵。同理，可以计算出样本属于 $C_2$ 类的后验概率。根据贝叶斯决策准则，将样本分配到后验概率较大的那个类别中。

这是基于正态分布的贝叶斯分类器的一般计算方法。但是，在特定情况下，判别函数可以化简为特例计算。例如，当两个类别的协方差矩阵相等时，即 $\Sigma_1 = \Sigma_2 = \Sigma$，可以将判别函数化简为：

$$g_i(\boldsymbol{x}) = \boldsymbol{x}^T\Sigma^{-1}\boldsymbol{\mu}_i - \frac{1}{2}\boldsymbol{\mu}_i^T\Sigma^{-1}\boldsymbol{\mu}_i + \ln p(C_i)$$

这个判别函数被称为线性判别函数，因为它是 $\boldsymbol{x}$ 的线性函数。当协方差矩阵相等时，意味着两个类别在特征空间中的分布形状相同，因此可以用一个线性超平面来划分两个类别。







在正态分布的贝叶斯决策问题中，已知两个类别 $C_1$ 和 $C_2$ 的先验分布 $p(C_1)$ 和 $p(C_2)$，以及它们的条件概率分布都服从正态分布，即：

$$p(\boldsymbol{x}|C_i) = \frac{1}{(2\pi)^{d/2}|\boldsymbol{\Sigma}_i|^{1/2}}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_i)^T\boldsymbol{\Sigma}_i^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_i)\right),\ i=1,2$$

其中，$d$ 表示特征维度，$\boldsymbol{\mu}_i$ 和 $\boldsymbol{\Sigma}_i$ 分别是类别 $C_i$ 的均值向量和协方差矩阵。现在需要计算样本 $\boldsymbol{x}$ 属于类别 $C_1$ 和 $C_2$ 的条件概率 $p(C_1|\boldsymbol{x})$ 和 $p(C_2|\boldsymbol{x})$。

根据贝叶斯公式，可以得到：

$$p(C_i|\boldsymbol{x}) = \frac{p(\boldsymbol{x}|C_i)p(C_i)}{p(\boldsymbol{x})},\ i=1,2$$

其中，$p(\boldsymbol{x}) = p(\boldsymbol{x}|C_1)p(C_1) + p(\boldsymbol{x}|C_2)p(C_2)$ 是样本 $\boldsymbol{x}$ 的边缘概率密度函数。

将上面的正态分布式子代入贝叶斯公式中，可以得到：









在正态分布的Bayes决策问题中，已知两个类别的先验分布，以及条件概率分布服从正态分布，可以使用贝叶斯公式计算样本 $x$ 的条件概率。

假设样本 $x$ 属于类别 $C_i$，则其条件概率为：

$$p(x|C_i) = \frac{1}{(2\pi)^{d/2}|\Sigma_i|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_i)^T\Sigma_i^{-1}(x-\mu_i)\right)$$

其中，$d$ 表示样本的特征维度，$\mu_i$ 和 $\Sigma_i$ 分别是类别 $C_i$ 的均值向量和协方差矩阵。根据贝叶斯公式，可以计算样本属于类别 $C_i$ 的后验概率：

$$p(C_i|x) = \frac{p(x|C_i)p(C_i)}{p(x|C_1)p(C_1)+p(x|C_2)p(C_2)}$$

其中，$p(C_i)$ 是类别 $C_i$ 的先验概率，可以根据训练集中每个类别的样本数量计算得到。$p(x|C_1)$ 和 $p(x|C_2)$ 分别是样本 $x$ 在类别 $C_1$ 和 $C_2$ 中的条件概率。

将样本分配到后验概率较大的类别中即可完成分类。如果后验概率相等，则可以将样本分配到先验概率较大的类别中。



## 课件例题解答

根据正态分布的Bayes决策，需要比较两个类别的后验概率，将样本分配给后验概率较大的那个类别。具体地，对于二分类问题，假设类别$w_1$和类别$w_2$，样本$x$的观测值为$x_0$，则有以下计算公式：

$$P(w_1|x_0) = \frac{p(x_0|w_1)P(w_1)}{p(x_0|w_1)P(w_1)+p(x_0|w_2)P(w_2)}$$

$$P(w_2|x_0) = \frac{p(x_0|w_2)P(w_2)}{p(x_0|w_1)P(w_1)+p(x_0|w_2)P(w_2)}$$

其中，$p(x|w)$是样本$x$在类别$w$中的条件概率密度函数，$P(w)$是类别$w$的先验概率。对于服从正态分布的情况，条件概率密度函数可以表示为：

$$p(x|w)=\frac{1}{\sqrt{2\pi}\sigma_w}e^{-\frac{(x-\mu_w)^2}{2\sigma_w^2}}$$

其中，$\mu_w$是类别$w$的均值，$\sigma_w$是类别$w$的标准差。

将已知的数据代入公式计算后验概率，得到：

$$P(w_1|x=3100) = \frac{1}{\sqrt{2\pi}\times 1000}e^{-\frac{(3100-2000)^2}{2\times 1000^2}} \times P(w_1)$$

$$P(w_2|x=3100) = \frac{1}{\sqrt{2\pi}\times 3000}e^{-\frac{(3100-7000)^2}{2\times 3000^2}} \times P(w_2)$$

代入数据，假设先验概率$P(w_1)=P(w_2)=0.5$，则有：

$$P(w_1|x=3100) \approx 0.1067$$

$$P(w_2|x=3100) \approx 0.8933$$

因此，根据后验概率较大的原则，将样本$x=3100$判定为类别$w_2$。



## 连续型变量

使用高斯混合模型（Gaussian Mixture Model，GMM）对连续型变量做贝叶斯决策的过程可以分为以下几步：

1. 选择合适的GMM模型参数

在使用GMM进行贝叶斯决策之前，需要先选择合适的GMM模型参数。通常需要选择高斯分量数目、每个高斯分量的均值、协方差矩阵和每个高斯分量的权重。

1. 训练GMM模型

使用选定的GMM模型参数，对训练数据进行训练，从而得到训练好的GMM模型。

1. 计算后验概率

对于每个测试样本，使用训练好的GMM模型计算其对应每个类别的后验概率。具体地，根据贝叶斯定理：

$$P(y|x) = \frac{P(x|y)P(y)}{P(x)}$$

其中，$y$表示类别，$x$表示样本特征，$P(y|x)$表示给定特征$x$时，样本属于类别$y$的概率，$P(x|y)$表示在类别$y$下样本特征$x$的条件概率，$P(y)$表示类别$y$的先验概率，$P(x)$表示样本特征$x$的边缘概率。对于GMM模型，可以使用后验概率公式：

$$P(x|y) = \sum_{k=1}^{K} \alpha_{y,k} \mathcal{N}(x|\mu_{y,k},\Sigma_{y,k})$$

其中，$\alpha_{y,k}$表示类别$y$下第$k$个高斯分量的权重，$\mu_{y,k}$表示类别$y$下第$k$个高斯分量的均值，$\Sigma_{y,k}$表示类别$y$下第$k$个高斯分量的协方差矩阵，$\mathcal{N}(x|\mu_{y,k},\Sigma_{y,k})$表示$x$在类别$y$下第$k$个高斯分量的概率密度函数。

1. 做出决策

根据计算出的后验概率，选择概率最大的类别作为测试样本的预测类别，即：

$$\hat{y} = \arg\max_y P(y|x)$$

以上就是使用GMM模型对连续型变量做贝叶斯决策的过程。

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import f1_score, jaccard_score, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建自定义数据集类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BreastCancerSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    乳腺癌分割数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, one_hot_encode=True, target_size=(512, 512)):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.one_hot_encode = one_hot_encode\n",
    "        self.target_size = target_size\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据文件存放方式设置os，便于建立原始图像与mask图像的联系\n",
    "        img_name = self.img_filenames[index]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name[:-4] + '_mask'+img_name[-4:])\n",
    "        # 跳过 .ipynb_checkpoints 文件\n",
    "        if img_path.endswith(\".ipynb_checkpoints\") or mask_path.endswith(\".ipynb_checkpoints\"):\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # 适应度处理，检查是否成功将图像添加入os\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Mask not found at {mask_path}\")\n",
    "\n",
    "        # 将 image 和 mask 的图像进行强制转化，转化成同样大小\n",
    "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # 将mask进行独热处理\n",
    "        if self.one_hot_encode:\n",
    "            mask = self.one_hot_encoding(mask, num_classes=3)\n",
    "\n",
    "        # 定义transform构架，为数据增强做准备\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # 将mask的属性值转化为float类型\n",
    "        mask = np.asarray(mask)   # 转换为NumPy数组\n",
    "        mask = mask.astype(np.float32)   # 变换dtype\n",
    "        mask = torch.from_numpy(mask) # 转换为Tensor\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def one_hot_encoding(self, mask, num_classes):\n",
    "        one_hot = np.zeros((mask.shape[0], mask.shape[1], num_classes), dtype=np.uint8)\n",
    "        for c in range(num_classes):\n",
    "            one_hot[..., c] = (mask == c)\n",
    "        return one_hot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BreastCancerClassificationDataset(Dataset):\n",
    "    \"\"\"乳腺癌分类数据集\"\"\"\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "        self.target_size = (512, 512)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_filenames[index]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        if img_path.endswith(\".ipynb_checkpoints\"):\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "\n",
    "\n",
    "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if 'benign' in img_name:\n",
    "            label = 0\n",
    "        elif 'malignant' in img_name:\n",
    "            label = 1\n",
    "        elif 'normal' in img_name:\n",
    "            label = 2\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用自定义数据集类加载数据并进行数据增强\n",
    "划分数据集为训练集和验证集和测试集，定义数据加载器\n",
    "\n",
    "在这里我详细介绍一下图像的变化：\n",
    "\n",
    "1. 从CV2读取的原始图像是一个高度×宽度×通道数（H×W×C）的NumPy数组，其中通道数为3（红、绿、蓝通道）。数据类型为uint8，数值范围为[0, 255]。\n",
    "\n",
    "2. 在train_transform中，数据增强操作会按顺序应用于图像。例如，根据给定的概率，图像可能被翻转、旋转、调整亮度和对比度等。这些操作可能会改变图像的形状（如旋转后的尺寸）或颜色（如调整亮度和对比度）。\n",
    "\n",
    "3. A.Normalize()操作将图像数据归一化到零均值和单位方差。这有助于提高模型的收敛速度和性能。它不会改变图像的数据类型或形状。\n",
    "\n",
    "4. ToTensorV2()操作将NumPy数组转换为PyTorch张量。在转换过程中，数据类型将从uint8更改为float32，并且数值范围会从[0, 255]更改为[0, 1]。此外，形状也会发生变化：从高度×宽度×通道数（H×W×C）变为通道数×高度×宽度（C×H×W），以便于PyTorch处理。\n",
    "\n",
    "经过这些操作后，最终得到一个C×H×W的float32类型的PyTorch张量，数值范围为[0, 1]。这种格式适用于输入到深度学习模型中。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练集的transmform构架，为了增强泛化能力，进行了水平翻转，随机亮度对比度调整，随机旋转，随机裁剪和缩放，归一化，将图像数据转换为PyTorch张量等操作\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=30, p=0.3),\n",
    "    A.RandomResizedCrop(height=512, width=512, scale=(0.8, 1.0), p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 构建数据集字典，便于数据加载\n",
    "SegmentationDataset = {\n",
    "    'train': BreastCancerSegmentationDataset(\"image\", \"mask\", transform=train_transform),\n",
    "    'test': BreastCancerSegmentationDataset(\"image\", \"mask\", transform=test_transform)\n",
    "}\n",
    "# 构建训练集和测试集的 DataLoader\n",
    "S_train_loader = DataLoader(SegmentationDataset['train'], batch_size=16, shuffle=True, num_workers=8, drop_last=True)\n",
    "S_test_loader = DataLoader(SegmentationDataset['test'], batch_size=32, shuffle=False, num_workers=8, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ClassificationDataset = {\n",
    "    'train': BreastCancerClassificationDataset(\"image\", transform=train_transform),\n",
    "    'test': BreastCancerClassificationDataset(\"image\", transform=test_transform)\n",
    "}\n",
    "\n",
    "C_train_loader = DataLoader(ClassificationDataset['train'], batch_size=32, shuffle=True, num_workers=8, drop_last=True)\n",
    "C_test_loader = DataLoader(ClassificationDataset['test'], batch_size=32, shuffle=False, num_workers=8, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UNetTrainer:\n",
    "    \"\"\"实际上我们使用的是一个全卷积网络（FCN）的ResNet50实现，而不是U-Net\"\"\"\n",
    "    def __init__(self, num_classes=3, lr=1e-4):\n",
    "        # 我们使用的fcn_resnet50是U-Net模型的变体,其中编码器部分初始化了ResNet50的权重。这可以加速模型训练和提高最终性能。但解码器部分仍需要我们从零训练\n",
    "        self.model = fcn_resnet50(pretrained=False, num_classes=num_classes)\n",
    "        # 损失函数使用交叉熵损失函数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        # 使用adam优化器\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        # 设置GPU为训练设备\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.5)  # 设置学习率衰减的调度器，每过5次迭代学习率衰减0.5\n",
    "\n",
    "    def calculate_iou(self, pred, target):\n",
    "        \"\"\"IoU计算函数\"\"\"\n",
    "        return jaccard_score(target.cpu().numpy().ravel(), pred.cpu().numpy().ravel(), average=\"macro\")\n",
    "\n",
    "    def evaluate(self, dataloader, phase):\n",
    "        \"\"\"评估函数\"\"\"\n",
    "        if phase == \"train\":\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        # 初始化结果参数\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1_score = 0.0\n",
    "\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "\n",
    "            # 这里将mask中每个像素的最大值所在的通道作为像素的类别，并将mask转换为long类型。这样处理可以保持输出preds与masks的维度匹配，以便后续计算损失、IoU和F1分数。\n",
    "            # masks = torch.mean(masks, dim=3, keepdim=False).long() 取均值会引起精度异常高，并不是真的高，是过于乐观了，骗骗小孩子的那种(*^_^*)\n",
    "            masks = torch.argmax(masks, dim=3, keepdim=False).long()\n",
    "\n",
    "            # 梯度清零\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                # 得出模型预测结果\n",
    "                outputs = self.model(images)['out']\n",
    "                preds = torch.argmax(outputs, dim=1)  # issue：你他妈不觉得这里和上边的masks对不上号？\n",
    "                # 计算损失值\n",
    "                loss = self.criterion(outputs, masks)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_iou += self.calculate_iou(preds, masks.data)\n",
    "            running_f1_score += f1_score(masks.cpu().numpy().ravel(), preds.cpu().numpy().ravel(), average=\"macro\")\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_iou = running_iou / len(dataloader)\n",
    "        epoch_f1_score = running_f1_score / len(dataloader)\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} IoU: {epoch_iou:.4f} F1: {epoch_f1_score:.4f}\")\n",
    "\n",
    "\n",
    "    def train(self, num_epochs, train_loader, test_loader=None):\n",
    "        \"\"\"模型训练函数\"\"\"\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.evaluate(train_loader, \"train\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Epoch time: {elapsed_time:.4f}s\")\n",
    "            self.scheduler.step()  # 学习率衰减\n",
    "\n",
    "            if (epoch + 1) % 5==0 and test_loader:\n",
    "                self.evaluate(test_loader, \"test\")\n",
    "\n",
    "        print(\"Training complete\")\n",
    "\n",
    "\n",
    "        torch.save(self.model.state_dict(), 'S_trained_model.pth')\n",
    "\n",
    "\n",
    "    def test(self, test_loader, model_path='S_trained_model.pth'):\n",
    "        \"\"\"模型测试函数\"\"\"\n",
    "        # 加载模型\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        # 评估模型在测试集上的性能，即估计模型的泛化性能\n",
    "        print(\"Evaluating the model on the test dataset\")\n",
    "        self.evaluate(test_loader, \"test\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "实例化U-Net模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResNetTrainer:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.num_classes = 3\n",
    "        self.learning_rate = learning_rate\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.model = resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, self.num_classes)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        self.patience = 3  # 设定连续多少个epochs未出现验证损失改善时停止训练\n",
    "        self.best_val_loss = float(\"inf\")  # 初始化最佳验证损失为无穷大\n",
    "\n",
    "    def early_stopping(self, current_val_loss):\n",
    "        \"\"\"早停法\"\"\"\n",
    "        if current_val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "\n",
    "        if self.patience_counter >= self.patience:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def train(self, epochs, train_loader):\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            if self.early_stopping(running_loss):  # 使用验证损失调用早停法\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "            print(f\"Epoch: {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "        torch.save(self.model.state_dict(), 'C_trained_model.pth')\n",
    "\n",
    "    def test(self, test_loader):\n",
    "        self.model.eval()\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "                predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(report)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Strainer = UNetTrainer()\n",
    "Strainer.train(20, S_train_loader, S_test_loader)\n",
    "Strainer.test(S_test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Ctrainer = ResNetTrainer(0.001)\n",
    "Ctrainer.train(20, C_train_loader)\n",
    "Ctrainer.test(C_test_loader)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

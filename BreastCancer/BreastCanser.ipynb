{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:43.793070Z",
     "end_time": "2023-04-29T17:12:45.466810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建自定义数据集类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:45.459805Z",
     "end_time": "2023-04-29T17:12:46.139789Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "Transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    # other transforms, if needed\n",
    "])\n",
    "\n",
    "class BreastCancerSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    乳腺癌分割数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, one_hot_encode=True, target_size=(256, 256)):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.one_hot_encode = one_hot_encode\n",
    "        self.target_size = target_size\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_filenames[index]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name[:-4] + '_mask'+img_name[-4:])\n",
    "        # Skip .ipynb_checkpoints files\n",
    "        if img_path.endswith(\".ipynb_checkpoints\") or mask_path.endswith(\".ipynb_checkpoints\"):\n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Mask not found at {mask_path}\")\n",
    "\n",
    "        # Resize image and mask\n",
    "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if self.one_hot_encode:\n",
    "            mask = one_hot_encode(mask, num_classes=3)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        mask = np.asarray(mask)   # 转换为NumPy数组\n",
    "        mask = mask.astype(np.float32)   # 变换dtype\n",
    "        mask = torch.from_numpy(mask) # 转换为Tensor\n",
    "        return image, mask\n",
    "\n",
    "class BreastCancerClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    乳腺癌分类数据集\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_filenames = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.img_filenames[index]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found at {img_path}\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "\n",
    "\n",
    "        # 从图像名称解析分类标签\n",
    "        if 'benign' in img_name:\n",
    "            label = 0\n",
    "        elif 'malignant' in img_name:\n",
    "            label = 1\n",
    "        elif 'normal' in img_name:\n",
    "            label = 2\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def one_hot_encode(mask, num_classes):\n",
    "    one_hot = np.zeros((mask.shape[0], mask.shape[1], num_classes), dtype=np.uint8)\n",
    "    for c in range(num_classes):\n",
    "        one_hot[..., c] = (mask == c)\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用自定义数据集类加载数据并进行数据增强\n",
    "划分数据集为训练集和验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:46.124031Z",
     "end_time": "2023-04-29T17:12:47.982829Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-2.0665,  0.3823, -1.3302,  ...,  1.6667,  1.7523,  1.6495],\n          [-2.1008, -1.9124, -1.3302,  ...,  1.7009,  1.6495,  1.5639],\n          [ 0.9817,  1.4440, -1.5185,  ...,  1.4440,  1.5468,  1.4954],\n          ...,\n          [-1.7412, -1.6727, -1.7583,  ..., -1.6727, -1.6384, -1.5357],\n          [-1.7069, -1.6555, -1.7069,  ..., -1.7069, -1.6898, -1.7240],\n          [-1.8268, -1.7925, -1.7069,  ..., -1.6213, -1.5870, -1.6384]],\n \n         [[-1.9832,  0.5203, -1.2304,  ...,  1.8333,  1.9209,  1.8158],\n          [-2.0182, -1.8256, -1.2304,  ...,  1.8683,  1.8158,  1.7283],\n          [ 1.1331,  1.6057, -1.4230,  ...,  1.6057,  1.7108,  1.6583],\n          ...,\n          [-1.6506, -1.5805, -1.6681,  ..., -1.5805, -1.5455, -1.4405],\n          [-1.6155, -1.5630, -1.6155,  ..., -1.6155, -1.5980, -1.6331],\n          [-1.7381, -1.7031, -1.6155,  ..., -1.5280, -1.4930, -1.5455]],\n \n         [[-1.7522,  0.7402, -1.0027,  ...,  2.0474,  2.1346,  2.0300],\n          [-1.7870, -1.5953, -1.0027,  ...,  2.0823,  2.0300,  1.9428],\n          [ 1.3502,  1.8208, -1.1944,  ...,  1.8208,  1.9254,  1.8731],\n          ...,\n          [-1.4210, -1.3513, -1.4384,  ..., -1.3513, -1.3164, -1.2119],\n          [-1.3861, -1.3339, -1.3861,  ..., -1.3861, -1.3687, -1.4036],\n          [-1.5081, -1.4733, -1.3861,  ..., -1.2990, -1.2641, -1.3164]]]),\n tensor([[[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]],\n \n         [[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]],\n \n         [[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]],\n \n         ...,\n \n         [[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]],\n \n         [[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]],\n \n         [[1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.],\n          ...,\n          [1., 0., 0.],\n          [1., 0., 0.],\n          [1., 0., 0.]]]))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=30, p=0.3),\n",
    "    A.RandomResizedCrop(height=256, width=256, scale=(0.8, 1.0), p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "dataset = {\n",
    "    'train': BreastCancerSegmentationDataset(\"image\", \"mask\", transform=train_transform),\n",
    "    'val': BreastCancerSegmentationDataset(\"image\", \"mask\", transform=val_transform),\n",
    "    'test': BreastCancerSegmentationDataset(\"image\", \"mask\", transform=test_transform)\n",
    "}\n",
    "\n",
    "# 打印一个样本，检查数据格式和变换是否符合预期\n",
    "dataset['train'][10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:47.982829Z",
     "end_time": "2023-04-29T17:12:47.998915Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 构建训练集、验证集和测试集的 DataLoader\n",
    "train_loader = DataLoader(dataset['train'], batch_size=16, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(dataset['val'], batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n",
    "test_loader = DataLoader(dataset['test'], batch_size=32, shuffle=False, num_workers=0, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化U-Net模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:48.007373Z",
     "end_time": "2023-04-29T17:12:48.062653Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "class UNetTrainer:\n",
    "    def __init__(self, num_classes=3, lr=1e-4):\n",
    "        self.model = self.build_model(num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def build_model(self, num_classes):\n",
    "        model = fcn_resnet50(pretrained=False, num_classes=num_classes)\n",
    "        return model\n",
    "\n",
    "    def evaluate(self, epoch, dataloader, phase):\n",
    "        if phase == \"train\":\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        running_f1_score = 0.0\n",
    "\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "\n",
    "            # 由于数据集中的mask是三维的，需要转换为二维\n",
    "            masks = torch.mean(masks, dim=3, keepdim=False).long()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = self.model(images)['out']\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == masks.data)\n",
    "            running_f1_score += f1_score(masks.cpu().numpy().ravel(), preds.cpu().numpy().ravel(), average=\"macro\")\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "        epoch_f1_score = running_f1_score / len(dataloader)\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} F1: {epoch_f1_score:.4f}\")\n",
    "\n",
    "    def train(self, num_epochs, train_loader, val_loader):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.evaluate(epoch, train_loader, \"train\")\n",
    "            self.evaluate(epoch, val_loader, \"val\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Epoch time: {elapsed_time:.4f}s\")\n",
    "\n",
    "        print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "\n",
    "class UNetTrainer:\n",
    "    def __init__(self, num_classes=3, lr=1e-4):\n",
    "        self.model = self.build_model(num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def build_model(self, num_classes):\n",
    "        model = fcn_resnet50(pretrained=False, num_classes=num_classes)\n",
    "        return model\n",
    "\n",
    "    def calculate_iou(self, pred, target):\n",
    "        return jaccard_score(target.cpu().numpy().ravel(), pred.cpu().numpy().ravel(), average=\"macro\")\n",
    "\n",
    "    def evaluate(self, epoch, dataloader, phase):\n",
    "        if phase == \"train\":\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        running_f1_score = 0.0\n",
    "\n",
    "        for images, masks in dataloader:\n",
    "            images = images.to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "\n",
    "            masks = torch.mean(masks, dim=3, keepdim=False).long()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = self.model(images)['out']\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                loss = self.criterion(outputs, masks)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_iou += self.calculate_iou(preds, masks.data)\n",
    "            running_f1_score += f1_score(masks.cpu().numpy().ravel(), preds.cpu().numpy().ravel(), average=\"macro\")\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        epoch_iou = running_iou / len(dataloader)\n",
    "        epoch_f1_score = running_f1_score / len(dataloader)\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} IoU: {epoch_iou:.4f} F1: {epoch_f1_score:.4f}\")\n",
    "\n",
    "    def train(self, num_epochs, train_loader, val_loader):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(\"-\" * 10)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            self.evaluate(epoch, train_loader, \"train\")\n",
    "            self.evaluate(epoch, val_loader, \"val\")\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Epoch time: {elapsed_time:.4f}s\")\n",
    "\n",
    "        print(\"Training complete\")\n",
    "\n",
    "    def test(self, train_loader, val_loader, test_loader):\n",
    "        # Combine train and val datasets\n",
    "        combined_dataset = torch.utils.data.ConcatDataset([train_loader.dataset, val_loader.dataset])\n",
    "        combined_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=train_loader.batch_size, shuffle=True)\n",
    "\n",
    "        # Retrain the model on the combined dataset\n",
    "        print(\"Retraining the model on the combined dataset\")\n",
    "        self.train(len(train_loader.dataset) + len(val_loader.dataset), combined_loader, test_loader)\n",
    "\n",
    "        # Evaluate the model on the test dataset\n",
    "        print(\"Evaluating the model on the test dataset\")\n",
    "        self.evaluate(0, test_loader, \"test\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-29T17:12:48.016691Z",
     "end_time": "2023-04-29T17:12:48.078667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86177\\miniconda3\\envs\\D2L\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\86177\\miniconda3\\envs\\D2L\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m UNetTrainer()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtest(train_loader, val_loader, test_loader)\n",
      "Cell \u001B[1;32mIn[6], line 62\u001B[0m, in \u001B[0;36mUNetTrainer.train\u001B[1;34m(self, num_epochs, train_loader, val_loader)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     60\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 62\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluate(epoch, val_loader, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     65\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "Cell \u001B[1;32mIn[6], line 42\u001B[0m, in \u001B[0;36mUNetTrainer.evaluate\u001B[1;34m(self, epoch, dataloader, phase)\u001B[0m\n\u001B[0;32m     39\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, masks)\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m phase \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 42\u001B[0m         \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     45\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\D2L\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\D2L\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer = UNetTrainer()\n",
    "trainer.train(20, train_loader, val_loader)\n",
    "trainer.test(train_loader, val_loader, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
